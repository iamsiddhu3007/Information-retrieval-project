{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "#create the csv file of corpus and do preprocessing of documents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from langdetect import detect"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import glob\n",
    "import nltk\n",
    "import math\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "#case folding\n",
    "def case_folding(lines):\n",
    "  lines=lines.casefold()\n",
    "  return lines\n",
    "#removing punctuations\n",
    "def rm_punctuation(words):\n",
    "  words=words.translate(str.maketrans('','',string.punctuation))\n",
    "  return words\n",
    "#tkenization\n",
    "def tokenization(words):\n",
    "  token=nltk.word_tokenize(words)\n",
    "  return token\n",
    "#removing single letter tokens\n",
    "def rm_single(token):\n",
    "  for i in token:\n",
    "    if len(i)==1:\n",
    "      token.remove(i)\n",
    "  return token\n",
    "#removing stop words\n",
    "def rm_stop_words(token):\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  for word in token:\n",
    "    if word in stop_words:\n",
    "        token.remove(word)\n",
    "  return token\n",
    "#stemming of tokens\n",
    "def stemming(token):\n",
    "  porter = PorterStemmer()\n",
    "  for i in range(0,len(token)):\n",
    "    token[i]=porter.stem(token[i])\n",
    "  return token\n",
    "#lemmatizing of tokens\n",
    "def lemmatizing(token):\n",
    "  for i in range(0,len(token)):\n",
    "    token[i]=WordNetLemmatizer().lemmatize(token[i])\n",
    "  return token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def pre_processing(text):\n",
    "    text=case_folding(text)\n",
    "    text=rm_punctuation(text)\n",
    "    tokens=tokenization(text)\n",
    "    tokens=rm_single(tokens)\n",
    "    tokens=rm_stop_words(tokens)\n",
    "    tokens=stemming(tokens)\n",
    "    tokens=lemmatizing(tokens)\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham mail\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# load the model and the feature extraction object\n",
    "with open('spam_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "with open('vectorizer.pkl', 'rb') as file:\n",
    "    feature_extraction = pickle.load(file)\n",
    "\n",
    "# define new input text\n",
    "new_input = [\"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times\"]\n",
    "\n",
    "# transform the new input text to feature vectors using the saved feature extraction object\n",
    "new_input_features = feature_extraction.transform(new_input)\n",
    "\n",
    "# make predictions on the new input using the trained model\n",
    "predictions = model.predict(new_input_features)\n",
    "\n",
    "if predictions[0] == 1:\n",
    "    print('Ham mail')\n",
    "else:\n",
    "    print('Spam mail')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "v={35: 'U.S. NEWS', 5: 'COMEDY', 22: 'PARENTING', 40: 'WORLD NEWS', 7: 'CULTURE & ARTS', 32: 'TECH', 28: 'SPORTS', 10: 'ENTERTAINMENT', 24: 'POLITICS', 37: 'WEIRD NEWS', 11: 'ENVIRONMENT', 9: 'EDUCATION', 6: 'CRIME', 27: 'SCIENCE', 38: 'WELLNESS', 3: 'BUSINESS', 30: 'STYLE & BEAUTY', 13: 'FOOD & DRINK', 20: 'MEDIA', 25: 'QUEER VOICES', 17: 'HOME & LIVING', 39: 'WOMEN', 2: 'BLACK VOICES', 34: 'TRAVEL', 21: 'MONEY', 26: 'RELIGION', 19: 'LATINO VOICES', 18: 'IMPACT', 36: 'WEDDINGS', 4: 'COLLEGE', 23: 'PARENTS', 1: 'ARTS & CULTURE', 29: 'STYLE', 15: 'GREEN', 31: 'TASTE', 16: 'HEALTHY LIVING', 33: 'THE WORLDPOST', 14: 'GOOD NEWS', 41: 'WORLDPOST', 12: 'FIFTY', 0: 'ARTS', 8: 'DIVORCE'}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_7428\\1684259422.py:18: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  if detect(tweet.content)=='en':\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_7428\\1684259422.py:19: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  spam_prediction = spam_model.predict(feature_extraction.transform([tweet.content]))\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_7428\\1684259422.py:24: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  category=v[category_model.predict([tweet.content])[0]]\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_7428\\1684259422.py:25: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tokens=pre_processing(tweet.content)\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_7428\\1684259422.py:26: DeprecatedFeatureWarning: username is deprecated, use user.username instead\n",
      "  tweets.append([tweet.date, tweet.username, tweet.content,tweet.url, tweet.user.followersCount, tweet.likeCount, tweet.retweetCount,category,spam_pre,tokens])\n",
      "C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_7428\\1684259422.py:26: DeprecatedFeatureWarning: content is deprecated, use rawContent instead\n",
      "  tweets.append([tweet.date, tweet.username, tweet.content,tweet.url, tweet.user.followersCount, tweet.likeCount, tweet.retweetCount,category,spam_pre,tokens])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Date             User  \\\n",
      "0                         Date             User   \n",
      "1    2023-04-10 17:35:34+00:00  vitiligosociety   \n",
      "2    2023-04-10 17:10:50+00:00      SojanTaz360   \n",
      "3    2023-04-10 17:05:00+00:00    ScottGraffius   \n",
      "4    2023-04-10 17:00:06+00:00      Garenthino1   \n",
      "..                         ...              ...   \n",
      "995  2023-03-30 14:21:41+00:00   smartedgeideas   \n",
      "996  2023-03-30 14:09:24+00:00       banafahgeo   \n",
      "997  2023-03-30 13:49:12+00:00  Learning_Movies   \n",
      "998  2023-03-30 13:23:47+00:00    SarniaJournal   \n",
      "999  2023-03-30 13:00:36+00:00  wakeuplegendary   \n",
      "\n",
      "                                                 Tweet  \\\n",
      "0                                                Tweet   \n",
      "1    Sonam, a beauty influencer from London, has bu...   \n",
      "2    and analyzing data to track performance and ma...   \n",
      "3    If you’d like to find out the lifespan (half-l...   \n",
      "4    Q: Can I use social media ads to promote my Yo...   \n",
      "..                                                 ...   \n",
      "995  An assumption is anything for which you don't ...   \n",
      "996  Social Media including Movie Night. Youtube is...   \n",
      "997  It’s apparently been a year since the last tim...   \n",
      "998  From gaming to Instagram, YouTube to Snapchat....   \n",
      "999  Creativity is intelligence having fun. Get cre...   \n",
      "\n",
      "                                                   Url  Followers  Likes  \\\n",
      "0                                                  Url  Followers  Likes   \n",
      "1    https://twitter.com/vitiligosociety/status/164...       4157      0   \n",
      "2    https://twitter.com/SojanTaz360/status/1645474...         10      0   \n",
      "3    https://twitter.com/ScottGraffius/status/16454...       8358      1   \n",
      "4    https://twitter.com/Garenthino1/status/1645471...          6      0   \n",
      "..                                                 ...        ...    ...   \n",
      "995  https://twitter.com/smartedgeideas/status/1641...       1483      1   \n",
      "996  https://twitter.com/banafahgeo/status/16414425...         38      0   \n",
      "997  https://twitter.com/Learning_Movies/status/164...        112      0   \n",
      "998  https://twitter.com/SarniaJournal/status/16414...       4186      1   \n",
      "999  https://twitter.com/wakeuplegendary/status/164...       2901      0   \n",
      "\n",
      "     Retweets        Category  Spam/Ham  \\\n",
      "0    Retweets        Category  Spam/Ham   \n",
      "1           0  STYLE & BEAUTY       Ham   \n",
      "2           0            TECH       Ham   \n",
      "3           0            TECH       Ham   \n",
      "4           0        BUSINESS       Ham   \n",
      "..        ...             ...       ...   \n",
      "995         0        WELLNESS       Ham   \n",
      "996         0          COMEDY       Ham   \n",
      "997         0            TECH       Ham   \n",
      "998         1       PARENTING       Ham   \n",
      "999         0       PARENTING       Ham   \n",
      "\n",
      "                                                tokens  \n",
      "0                                               tokens  \n",
      "1    [sonam, beauti, influenc, london, built, solid...  \n",
      "2    [analyz, data, track, perform, make, inform, d...  \n",
      "3    [you, like, find, lifespan, halflif, post, soc...  \n",
      "4    [use, social, medium, ad, promot, youtub, chan...  \n",
      "..                                                 ...  \n",
      "995  [assumpt, anyth, which, dont, actual, proof, l...  \n",
      "996  [social, medium, includ, movi, night, youtub, ...  \n",
      "997  [s, appar, year, sinc, last, time, share, grow...  \n",
      "998  [game, instagram, youtub, snapchat, youth, amp...  \n",
      "999  [creativ, intellig, fun, get, creativ, share, ...  \n",
      "\n",
      "[1000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "query = \"youtube social media instagram \"\n",
    "limit = 1000\n",
    "tweets = [['Date', 'User', 'Tweet', 'Url', 'Followers', 'Likes', 'Retweets','Category', 'Spam/Ham', 'tokens']]\n",
    "import pickle\n",
    "\n",
    "with open('classify_model.pkl', 'rb') as f:\n",
    "    category_model = pickle.load(f)\n",
    "\n",
    "with open('spam_model.pkl', 'rb') as f:\n",
    "    spam_model = pickle.load(f)\n",
    "\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    # print(vars(tweet))\n",
    "    # break\n",
    "    if len(tweets) == limit:\n",
    "        break\n",
    "    else:\n",
    "        if detect(tweet.content)=='en':\n",
    "            spam_prediction = spam_model.predict(feature_extraction.transform([tweet.content]))\n",
    "            if spam_prediction[0] == 1:\n",
    "                spam_pre='Ham'\n",
    "            else:\n",
    "                spam_pre='Spam'\n",
    "            category=v[category_model.predict([tweet.content])[0]]\n",
    "            tokens=pre_processing(tweet.content)\n",
    "            tweets.append([tweet.date, tweet.username, tweet.content,tweet.url, tweet.user.followersCount, tweet.likeCount, tweet.retweetCount,category,spam_pre,tokens])\n",
    "\n",
    "df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet','Url', 'Followers', 'Likes', 'Retweets','Category','Spam/Ham','tokens'])\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "df.to_csv('corpus.csv', mode='a', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date             User  \\\n",
      "0                       Date             User   \n",
      "1  2023-04-10 17:35:34+00:00  vitiligosociety   \n",
      "2  2023-04-10 17:10:50+00:00      SojanTaz360   \n",
      "3  2023-04-10 17:05:00+00:00    ScottGraffius   \n",
      "4  2023-04-10 17:00:06+00:00      Garenthino1   \n",
      "\n",
      "                                               Tweet  \\\n",
      "0                                              Tweet   \n",
      "1  Sonam, a beauty influencer from London, has bu...   \n",
      "2  and analyzing data to track performance and ma...   \n",
      "3  If you’d like to find out the lifespan (half-l...   \n",
      "4  Q: Can I use social media ads to promote my Yo...   \n",
      "\n",
      "                                                 Url  Followers  Likes  \\\n",
      "0                                                Url  Followers  Likes   \n",
      "1  https://twitter.com/vitiligosociety/status/164...       4157      0   \n",
      "2  https://twitter.com/SojanTaz360/status/1645474...         10      0   \n",
      "3  https://twitter.com/ScottGraffius/status/16454...       8358      1   \n",
      "4  https://twitter.com/Garenthino1/status/1645471...          6      0   \n",
      "\n",
      "   Retweets        Category  Spam/Ham  \\\n",
      "0  Retweets        Category  Spam/Ham   \n",
      "1         0  STYLE & BEAUTY       Ham   \n",
      "2         0            TECH       Ham   \n",
      "3         0            TECH       Ham   \n",
      "4         0        BUSINESS       Ham   \n",
      "\n",
      "                                              tokens  \n",
      "0                                             tokens  \n",
      "1  [sonam, beauti, influenc, london, built, solid...  \n",
      "2  [analyz, data, track, perform, make, inform, d...  \n",
      "3  [you, like, find, lifespan, halflif, post, soc...  \n",
      "4  [use, social, medium, ad, promot, youtub, chan...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
