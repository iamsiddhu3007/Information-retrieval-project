{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import string\n",
    "import glob\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def search(query):\n",
    "    filename = 'search_result.csv'\n",
    "    if os.path.isfile(filename):\n",
    "        with open(filename, 'w', newline='') as f:\n",
    "            f.write('')\n",
    "    nltk_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    def preprocess(text):\n",
    "        text = text.lower()\n",
    "        text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "        text = re.sub('[0-9]+', '', text)\n",
    "        text = text.strip()\n",
    "        tokens = re.split('\\W+', text)\n",
    "        tokens = [token for token in tokens if token != '' and len(token) > 1]\n",
    "        tokens = [token for token in tokens if token not in nltk_stopwords]\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return tokens\n",
    "    def calculate_idf(N: int, df: int) -> float:\n",
    "        '''\n",
    "        calculate the inverse document frequency (idf) of a term\n",
    "        :param N: number of documents\n",
    "        :param df: document frequency of a term (number of documents containing the term)\n",
    "        '''\n",
    "        if df == 0:\n",
    "            return 0\n",
    "        return math.log(N/df, 10)\n",
    "    def get_document_frequency(term: str, documents: list) -> int:\n",
    "        '''\n",
    "        calculate the document frequency of a term in a list of documents\n",
    "        :param term: the term\n",
    "        :param documents: the list of documents\n",
    "        '''\n",
    "        count = 0\n",
    "        for document in documents:\n",
    "            if term in document:\n",
    "                count += 1\n",
    "        return count\n",
    "    def log_term_frequency(freq: int) -> float:\n",
    "        '''\n",
    "        calculate the log term frequency of a term in a document\n",
    "        :param freq: the term frequency\n",
    "        '''\n",
    "        if freq == 0:\n",
    "            return 0\n",
    "        return 1 + math.log(freq, 10)\n",
    "    def preprocess_documents(df):\n",
    "        document_vectors = []\n",
    "        for document in df['Content_preprocessed']:\n",
    "            #Calculating the term frequency\n",
    "            term_frequency = {}\n",
    "            for term in document:\n",
    "                if term in term_frequency:\n",
    "                    term_frequency[term] += 1\n",
    "                else:\n",
    "                    term_frequency[term] = 1\n",
    "            #Calculating the log term frequency 1.a\n",
    "            for term in term_frequency:\n",
    "                term_frequency[term] = log_term_frequency(term_frequency[term])\n",
    "            #Calculating the document frequency 1.b, but not used because document frequency is not part of the weighting scheme\n",
    "            #Taking cosine normalization 1.c\n",
    "            sum_of_squares = sum([freq**2 for freq in term_frequency.values()])\n",
    "            sq_sum_of_squares = math.sqrt(sum_of_squares)\n",
    "            for term in term_frequency:\n",
    "                term_frequency[term] /= sq_sum_of_squares\n",
    "\n",
    "            document_vectors.append(term_frequency)\n",
    "\n",
    "        return document_vectors\n",
    "    def preprocess_query(query: str) -> list:\n",
    "        '''\n",
    "        preprocess the query\n",
    "        :param query: the query\n",
    "        '''\n",
    "\n",
    "        #Preprocessing the query\n",
    "        query = preprocess(query)\n",
    "        #Calculating the term frequency of the query\n",
    "        query_vector = {}\n",
    "        for term in query:\n",
    "            if term in query_vector:\n",
    "                query_vector[term] += 1\n",
    "            else:\n",
    "                query_vector[term] = 1\n",
    "        #Calculating the inverse document frequency of the query terms 2.b\n",
    "        idf = {}\n",
    "        for term in query_vector:\n",
    "            idf[term] = calculate_idf(len(df), get_document_frequency(term, df['Content_preprocessed']))\n",
    "        #Calculating the log term frequency of the query terms 2.a\n",
    "        for term in query_vector:\n",
    "            query_vector[term] = log_term_frequency(query_vector[term])\n",
    "        #Multiplying the term frequency with the inverse document frequency 2.b\n",
    "        for term in query_vector:\n",
    "            query_vector[term] = query_vector[term] * idf[term]\n",
    "\n",
    "        #Normalizing the query vector with cosine normalization 2.c\n",
    "        sum_of_squares = sum([freq**2 for freq in query_vector.values()])\n",
    "        sq_sum_of_squares = math.sqrt(sum_of_squares)\n",
    "        if sq_sum_of_squares == 0:\n",
    "            return query_vector\n",
    "        for term in query_vector:\n",
    "            query_vector[term] /= sq_sum_of_squares\n",
    "\n",
    "        return query_vector\n",
    "    def calculate_cosine_similarity(query_vector: dict, document_vector: dict) -> float:\n",
    "        '''\n",
    "        calculate the cosine similarity between a query and a document\n",
    "        :param query_vector: the query vector\n",
    "        :param document_vector: the document vector\n",
    "        '''\n",
    "        numerator = 0\n",
    "        for term in query_vector:\n",
    "            if term in document_vector:\n",
    "                numerator += query_vector[term] * document_vector[term]\n",
    "        return numerator\n",
    "    def get_top_k_documents(query: str, k: int) -> list:\n",
    "        '''\n",
    "        get the top k documents for a query\n",
    "        :param query: the query\n",
    "        :param k: number of documents to return\n",
    "        '''\n",
    "        output=[]\n",
    "        query_vector = preprocess_query(query)\n",
    "        scores = []\n",
    "        for document_vector in dc_vectors:\n",
    "            scores.append(calculate_cosine_similarity(query_vector, document_vector))\n",
    "        output=scores\n",
    "        print(scores)\n",
    "        #Printing the top k documents with their titles and scores\n",
    "        #Getting the indices of the top k documentslo\n",
    "        #tweet-content,twewt-likes,tweet-retweets,tweet-content.username,tweet url,followers,category\n",
    "        top_k_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
    "        for i in range(k):\n",
    "            print('\\n' + str(i+1) + ')\\nTitle: ', df.iloc[top_k_indices[i]]['Tweet'])\n",
    "            print('Score: ', scores[top_k_indices[i]], end=\"\\n---------------XXX---------------\")\n",
    "        return output\n",
    "    v={35: 'U.S. NEWS', 5: 'COMEDY', 22: 'PARENTING', 40: 'WORLD NEWS', 7: 'CULTURE & ARTS', 32: 'TECH', 28: 'SPORTS', 10: 'ENTERTAINMENT', 24: 'POLITICS', 37: 'WEIRD NEWS', 11: 'ENVIRONMENT', 9: 'EDUCATION', 6: 'CRIME', 27: 'SCIENCE', 38: 'WELLNESS', 3: 'BUSINESS', 30: 'STYLE & BEAUTY', 13: 'FOOD & DRINK', 20: 'MEDIA', 25: 'QUEER VOICES', 17: 'HOME & LIVING', 39: 'WOMEN', 2: 'BLACK VOICES', 34: 'TRAVEL', 21: 'MONEY', 26: 'RELIGION', 19: 'LATINO VOICES', 18: 'IMPACT', 36: 'WEDDINGS', 4: 'COLLEGE', 23: 'PARENTS', 1: 'ARTS & CULTURE', 29: 'STYLE', 15: 'GREEN', 31: 'TASTE', 16: 'HEALTHY LIVING', 33: 'THE WORLDPOST', 14: 'GOOD NEWS', 41: 'WORLDPOST', 12: 'FIFTY', 0: 'ARTS', 8: 'DIVORCE'}\n",
    "    limit = 1000\n",
    "    tweets = []\n",
    "    with open('classify_model.pkl', 'rb') as f:\n",
    "        category_model = pickle.load(f)\n",
    "\n",
    "    with open('spam_model.pkl', 'rb') as f:\n",
    "        spam_model = pickle.load(f)\n",
    "\n",
    "    with open('vectorizer.pkl', 'rb') as file:\n",
    "        feature_extraction = pickle.load(file)\n",
    "    for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "        # print(vars(tweet))\n",
    "        # break\n",
    "        if len(tweets) == limit:\n",
    "            break\n",
    "        else:\n",
    "            if detect(tweet.content)=='en':\n",
    "                spam_prediction = spam_model.predict(feature_extraction.transform([tweet.content]))\n",
    "                if spam_prediction[0] == 1:\n",
    "                    spam_pre='Ham'\n",
    "                else:\n",
    "                    spam_pre='Spam'\n",
    "                category=v[category_model.predict([tweet.content])[0]]\n",
    "                tweets.append([tweet.date, tweet.username, tweet.content,tweet.url, tweet.user.followersCount, tweet.likeCount, tweet.retweetCount,category,spam_pre])\n",
    "\n",
    "    df = pd.DataFrame(tweets, columns=['Date', 'User', 'Tweet','Url', 'Followers', 'Likes', 'Retweets','Category','Spam/Ham'])\n",
    "    df['Content_preprocessed'] = df['Tweet'].apply(preprocess)\n",
    "    dc_vectors = preprocess_documents(df)\n",
    "    output=get_top_k_documents(query, len(df))\n",
    "    df['Scores'] = output\n",
    "    df.to_csv('search_result.csv', mode='a', index=False, header=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
